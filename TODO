
User Guide:

  Write intro, installation, config, architecture sections.
  Write other subsections marked with 'TODO'.

Release blockers:

  Make slack interval configurable.

  The following causes a crash:
    SELECT COUNT(*) FROM foo OVER mywin
    WINDOW mywin AS (RANGE INTERVAL 10 SECONDS PRECEDING);




Aggregation:

  -- unit test, unit test, unit test, baby.

  ... for a bit later:

  -- Agg funcs cannot be used in broader expressions (Cannot select 1 + COUNT(x)).
     In cases where they are used like this, assign the function's output a new
     anonymous name, promote the aggfunc to the aggregate functions list,
     and replace the call to the agg func in the expression with an IdentExpr to
     look up the value of this function's output.

  -- Allow use of WINDOW clauses inside AliasedExprs, rather than having a single
     global GROUP BY?

  -- Implement SQL:2003 windowing functions like LEAD, LAG, RANK. See slides at
     http://wwwlgis.informatik.uni-kl.de/cms/fileadmin/courses/SS2008/NEDM/RDDM.Chapter.06.Windows_and_Query_Functions_in_SQL.pdf

  -- Also new scalar functions listed there.

  -- Allow additional expr arguments (where isConstant() is true) to parameterize calls
     to aggregate functions. e.g., LEAD(x, 2), LAG(y, 3), etc.

  -- CHECK: Do we ever need non-ConstExpr arguments? Do we ever need to aggregate over
     two columns at once?

Remote environment stuff:
  -- Design question: think about whether operations like ExecEnv.setFlowName() should be
     synchronous or async. They're currently the latter -- would users expect
     the former?
  -- Icing: Operations like \w <streamName> should call a method that resolves a name to
     a flowId, and then calls watch() on the flowId.

RPC Testing:
  ... test what happens with multiple clients listening to the same flow
  and what happens when one ^C's, etc.

  This requires being able to set the 'callback RPC port'. Really the best way to do
  this is to have a configurable default callback RPC port (so a sysadmin can bust a
  specific port open), but if that's busy/fails, pick a random port and send that to
  the server.

  

Joins:
  I don't know that the various calls to streamSymbol.getName() are actually
  the correct name here -- I think we want the user's alias to get its way
  into the various source types.

  SelectStmt should support WINDOW clauses at the end.
  ^-- need to eval them to WindowSpecs and store these bindings for use in joinedsrc
      evaluation to hashjoinnodes.


  .. what happens if we use the same stream alias in two different subtrees of
  a compound select stmt?
  
  SELECT ... FROM
      ((SELECT x FROM f JOIN g) AS s1)
  JOIN
      ((SELECT x FROM h AS f JOIN g) AS s2)

  Specifically w.r.t. concerns over the lists returned by getSourceNames().. does
  this stop at the SELECT level, or does it recursively gather?
  I think we're ok for SELECT/JOIN... but I don't know how we will do for predicate
  push-down later..

Expressions:
  - Refine user_sel rule (or TypeChecker) so that it cannot accept identifier names that
    end in "_". (We need these for internal field identifiers in avro.)


Technical debt:
  - Write more unit tests as outlined at the bottom of TestSelect.
  - Need more tests in TestTypeChecker; see TODO in comments.
  - LocalEnv / LocalEnvThread is full of SelectableQueue<Object> because we put
    both EventWrappers and ControlOps in different threads. We then typecase on
    what we get back. Would be nice if we could create some common interface
    with a dispatch() / delegate()-and-handle() model to clean up the big
    typecase in the run() method...
  - Select.join() will consult its underlying Selectables in a stable order,
    which means we might starve later Selectables in the list. We need to
    perturb this to ensure fairness.

SQL Features (in rough priority order):
  - Filtering (HAVING)
  - LIMIT (within a window?)
  - [windowed] ORDER BY -- how does this work for overlapping windows?

Bug in CREATE STREAM AS SELECT and Flume node management:

  - We currently assume the Flume world is fairly static. If we set up a stream to
    read from node 'x', we will create a new local logical node named 'x-receiver'.
    If 'x' disappears, we do not delete 'x-receiver'. But if 'x' later reappears,
    we never re-connect it to 'x-receiver'. The changes needed to handle this would
    be in ForeignNodeConn / EmbeddedFlumeConfig.
    ** Consider a watchdog thread associated with each receiver. If the upstream
       disappears, close the receiver and all associated flows.
    ** Even better: a watchdog that waits for the upstream node to disappear. If it
       then reappears, we re-configure it to again connect down to our x-receiver
       in addition to whatever new downstream source it has.

  - CSAS is currently modeled as creating a local output node for the stream; it has
    no connection to the downstream -receiver node and associated flows.
    ** We should not allow dropping a flow associated with CSAS as long as any flows
       require that flow to exist upstream.
    ** More broadly: We should not allow DROP STREAM if any running flows depend on
       that stream.


Bugs:
  - Quitting is very slow due to the Flume shutdown. Can we improve this?
    ... it also emits a scary looking error message, that we should suppress for
    hygeine's sake.

Features:

  - Windowing should operate on 'previous n rows' too.
    Associate a rowid stamp with every event on input.
    Start each stream at 0 for each query, join operates like a "zipper"

  - Need ability to run remote physical plan on a set of configured nodes.

  - EventParser names/implementations should be accessed through the BuiltInSymbolTable.
  - Need a MapFunc API to allow 1-to-many transformations.

  - Should be able to CREATE STREAM AS FILE and then specify the file format. Right now
    we can't practically use avro data in files, because we parse the file itself as
    if it is \n-terminated text records.

  - Would be nice to select arbitrary attributes from events with a syntax like
    '#attrname'.

Optimizations:

  - ProjectionNode instances with identical input and output schemata should be removed.
  - ProjectionNode immediately after NamedSource should be fused.
  -- longer term: projection (and filtering) should be pushed up into previous
     FlowElements, when we use DAG tiling.
  - AvroEventWrapper could be improved by deserializing only the fields necessary
    for a given combined set of FE's in advance, knowing the input, output, and also
    internal-use schemas. (We may take in <int, string> and emit <int, string> but
    only query the int component; we could project onto that as we deserialize, and
    then save ourselves the trouble on the string.)
  - Expr.eval() should go by the wayside; what we really want is each Expr contributing
    a set of three-addr codes to a set of basic blocks that corresponds to the set of
    expressions the user wants emitted; then we can perform further optimization on
    this like common subexpression elimination, etc. We need to define a RTL that can
    handle all this. Then if our opcodes are things like AddIntInt, AddFloatFloat, etc.,
    we can dispense with the stored Type instances inside Exprs being required at
    run time.


NOTABUG, but saved for future reference:
  A weird thing happens with AvroEventParser reading flows we generate in avro:
  ... This has been confirmed as a bug in Flume, not rtsql.

  # Create a bona fide source.
  create stream x (a string) from local source 'tail("/tmp/meep")';

  # Create a flow that reads all of this and emits it to a node 'foo'.
  create stream foo as select a from x;

  # Check its output schema syntax with an 'explain,' then create a stream named bar:
  create stream bar (a string) from node 'foo' event format 'avro' properties ('schema' =
  '{"type":"record","name":"foo","fields":[{"name":"a","type":["string","null"]}]}' );

  # Then finally, read from that:
  select * from bar;

  This chain of events causes Flume to configure a logical node, then reconfigure it.

  A bug:
  The first record that gets pushed through after the node is reconfigured to send
  its output to a new ephemeral logical node causes a big cascade of exceptions:

85342 [logicalNode foo-28] ERROR com.cloudera.flume.agent.LogicalNode  - Driver on foo closed LazyOpenSource | LazyOpenDecorator because of EventSink LazyOpenDecorator not open
java.lang.IllegalStateException: EventSink LazyOpenDecorator not open
  at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
  at com.cloudera.flume.core.EventSinkDecorator.append(EventSinkDecorator.java:56)
  at com.cloudera.flume.handlers.debug.LazyOpenDecorator.append(LazyOpenDecorator.java:71)
  at com.cloudera.flume.core.connector.DirectDriver$PumperThread.run(DirectDriver.java:93)
85344 [logicalNode foo-54] ERROR com.cloudera.flume.core.connector.DirectDriver  - Driving src/sink failed! LazyOpenSource | LazyOpenDecorator because next() called before open()
java.io.IOException: next() called before open()
  at com.odiago.rtengine.flume.RtsqlSource.next(RtsqlSource.java:57)
  at com.cloudera.flume.handlers.debug.LazyOpenSource.next(LazyOpenSource.java:53)
  at com.cloudera.flume.core.connector.DirectDriver$PumperThread.run(DirectDriver.java:89)

  so, there's an open() call maybe missing in flume after a reconfigure?
  The first record is lost. The second record works.

  LogicalNode is known to not be thread-safe. It may call next() before open() with
  an unsynchronized race.
