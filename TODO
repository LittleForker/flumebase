
Remote environment stuff:
  Need ability to send data to a Flume stream.
  -- Operations like \w <streamName> should call a method that resolves a name to
     a flowId, and then calls watch() on the flowId.
  -- need unit tests on AvroEventParser
  -- need unit tests that operate on derived streams.

  ===============================================
  weird happenings with AvroEventParser reading flows we generate in avro:

  # Create a bona fide source.
  create stream x (a string) from local source 'tail("/tmp/meep")';

  # Create a flow that reads all of this and emits it to a node 'foo'.
  create stream foo as select a from x;

  # Check its output schema syntax with an 'explain,' then create a stream named foo:
  create stream foo (a string) from node 'foo' event format 'avro' properties ('schema' =
  '{"type":"record","name":"foo","fields":[{"name":"a","type":["string","null"]}]}' );

  # Then finally, read from that:
  select * from foo;

  A bug:
  The first record that gets pushed through after the node is reconfigured to send
  its output to a new ephemeral logical node causes a big cascade of exceptions:

85342 [logicalNode foo-28] ERROR com.cloudera.flume.agent.LogicalNode  - Driver on foo closed LazyOpenSource | LazyOpenDecorator because of EventSink LazyOpenDecorator not open
java.lang.IllegalStateException: EventSink LazyOpenDecorator not open
  at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
  at com.cloudera.flume.core.EventSinkDecorator.append(EventSinkDecorator.java:56)
  at com.cloudera.flume.handlers.debug.LazyOpenDecorator.append(LazyOpenDecorator.java:71)
  at com.cloudera.flume.core.connector.DirectDriver$PumperThread.run(DirectDriver.java:93)
85344 [logicalNode foo-54] ERROR com.cloudera.flume.core.connector.DirectDriver  - Driving src/sink failed! LazyOpenSource | LazyOpenDecorator because next() called before open()
java.io.IOException: next() called before open()
  at com.odiago.rtengine.flume.RtsqlSource.next(RtsqlSource.java:57)
  at com.cloudera.flume.handlers.debug.LazyOpenSource.next(LazyOpenSource.java:53)
  at com.cloudera.flume.core.connector.DirectDriver$PumperThread.run(DirectDriver.java:89)

  so, there's an open() call maybe missing in flume after a reconfigure?
  The first record is lost. The second record works.

  ===============================================
  ... test what happens with multiple clients listening to the same flow
  and what happens when one ^C's, etc.

  ===============================================
  Problematic behavior
    client connects, sets up a flow
    data comes back
    client ^C
    send another message into the flow. it goes nowhere (expected)
    -- but no err reported back to the server about the lost output.
    connect another client
    watch the same flow
    send another message into the flow. 
    -- server sees an error writing to the first client (expected) and closes
    -- .... server does NOT send the message to the new client.
    send a 3rd message into the flow.
    -- seen by the new client.


    - The 2nd message should be delivered to the 2nd client.
    - The 1st message should have caused the error on the server side.

Joins:
  I don't know that the various calls to streamSymbol.getName() are actually
  the correct name here -- I think we want the user's alias to get its way
  into the various source types.

  SelectStmt should support WINDOW clauses at the end.
  ^-- need to eval them to WindowSpecs and store these bindings for use in joinedsrc
      evaluation to hashjoinnodes.


  .. what happens if we use the same stream alias in two different subtrees of
  a compound select stmt?
  
  SELECT ... FROM
      ((SELECT x FROM f JOIN g) AS s1)
  JOIN
      ((SELECT x FROM h AS f JOIN g) AS s2)

  Specifically w.r.t. concerns over the lists returned by getSourceNames().. does
  this stop at the SELECT level, or does it recursively gather?
  I think we're ok for SELECT/JOIN... but I don't know how we will do for predicate
  push-down later..

Expressions:
  - Refine user_sel rule (or TypeChecker) so that it cannot accept identifier names that
    end in "_". (We need these for internal field identifiers in avro.)


Technical debt:
  - Write more unit tests as outlined at the bottom of TestSelect.
  - Need more tests in TestTypeChecker; see TODO in comments.
  - LocalEnv / LocalEnvThread is full of SelectableQueue<Object> because we put
    both EventWrappers and ControlOps in different threads. We then typecase on
    what we get back. Would be nice if we could create some common interface
    with a dispatch() / delegate()-and-handle() model to clean up the big
    typecase in the run() method...
  - Select.join() will consult its underlying Selectables in a stable order,
    which means we might starve later Selectables in the list. We need to
    perturb this to ensure fairness.

SQL Features (in rough priority order):
  - Aggregation and aggregate functions
  - Filtering (HAVING)
  - LIMIT (within a window?)
  - [windowed] ORDER BY -- how does this work for overlapping windows?

Aggregation:

  - Implement COUNT(*), COUNT(foo).
  - Implement windowing.


Bugs:

  - Quitting is very slow due to the Flume shutdown. Can we improve this?
    ... it also emits a scary looking error message, that we should suppress for
    hygeine's sake.

Features:

  - Windowing should operate on 'previous n rows' too.
    Associate a rowid tamp with every event on input.
    Start each stream at 0 for each query, join operates like a "zipper"

  - Need ability to run remote physical plan on a set of configured nodes.

  - EventParser names/implementations should be accessed through the BuiltInSymbolTable.
  - Need a MapFunc API to allow 1-to-many transformations.


Optimizations:

  - ProjectionNode instances with identical input and output schemata should be removed.
  - ProjectionNode immediately after NamedSource should be fused.
  -- longer term: projection (and filtering) should be pushed up into previous
     FlowElements, when we use DAG tiling.
  - AvroEventWrapper could be improved by deserializing only the fields necessary
    for a given combined set of FE's in advance, knowing the input, output, and also
    internal-use schemas. (We may take in <int, string> and emit <int, string> but
    only query the int component; we could project onto that as we deserialize, and
    then save ourselves the trouble on the string.)
  - Expr.eval() should go by the wayside; what we really want is each Expr contributing
    a set of three-addr codes to a set of basic blocks that corresponds to the set of
    expressions the user wants emitted; then we can perform further optimization on
    this like common subexpression elimination, etc. We need to define a RTL that can
    handle all this. Then if our opcodes are things like AddIntInt, AddFloatFloat, etc.,
    we can dispense with the stored Type instances inside Exprs being required at
    run time.

